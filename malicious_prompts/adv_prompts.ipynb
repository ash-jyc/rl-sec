{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d0e3a9f",
   "metadata": {},
   "source": [
    "# Qwen2.5-Coder (Hugging Face)\n",
    "\n",
    "This notebook helps you install dependencies and load a lightweight Qwen2.5-Coder model locally on macOS. Default: `Qwen/Qwen2.5-Coder-1.5B-Instruct` for faster startup. We'll use CPU or Apple Silicon MPS automatically.\n",
    "\n",
    "Notes:\n",
    "- First load will download model weights (~3–4 GB). Ensure disk space and a stable connection.\n",
    "- You can switch to larger models later (7B, 32B, etc.) or try quantized GGUF via llama.cpp for lower RAM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ba9a7",
   "metadata": {},
   "source": [
    "## AdvBench dataset (walledai/AdvBench)\n",
    "\n",
    "This dataset is gated on Hugging Face. To access:\n",
    "- You must be logged in to Hugging Face and accept the dataset conditions on its page.\n",
    "- In a terminal, optionally login to persist credentials: `huggingface-cli login`\n",
    "- The code below will attempt to load via `datasets.load_dataset`. If access is denied, it will fall back to a public CSV from the paper’s GitHub mirror.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1fa596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download AdvBench into data/advbench\n",
    "import os, pathlib, json\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "root = pathlib.Path.cwd()\n",
    "data_dir = root / 'data' / 'advbench'\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Target directory:', data_dir)\n",
    "\n",
    "# Try Hugging Face datasets first (requires access acceptance)\n",
    "dset = None\n",
    "try:\n",
    "    dset = load_dataset('walledai/AdvBench')\n",
    "    # The dataset is small; save each split to CSV\n",
    "    for split in dset:\n",
    "        df = dset[split].to_pandas()\n",
    "        out = data_dir / f'{split}.csv'\n",
    "        df.to_csv(out, index=False)\n",
    "        print('Saved:', out)\n",
    "except Exception as e:\n",
    "    print('HF dataset access failed or gated; falling back to GitHub CSV. Error:', e)\n",
    "\n",
    "if dset is None:\n",
    "    # Fallback to the GitHub mirror from the paper\n",
    "    url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(url)\n",
    "        out = data_dir / 'harmful_behaviors.csv'\n",
    "        df.to_csv(out, index=False)\n",
    "        print('Saved fallback CSV:', out)\n",
    "    except Exception as e2:\n",
    "        raise RuntimeError(f'Failed to fetch AdvBench from both HF and GitHub: {e2}')\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install runtime deps. Run once per environment.\n",
    "import sys, subprocess, textwrap\n",
    "\n",
    "pkgs = [\n",
    "    'transformers>=4.44.0',\n",
    "    'torch',        # macOS CPU/MPS wheels\n",
    "    'accelerate>=0.33.0',\n",
    "    'sentencepiece',\n",
    "    'huggingface_hub>=0.23.0',\n",
    "    'datasets>=2.20.0'\n",
    "]\n",
    "\n",
    "print('Installing:', pkgs)\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade'] + pkgs)\n",
    "print('Done. You may need to restart kernel if torch backend was updated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24abba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 7B Instruct model\n",
    "import os\n",
    "os.environ['QWEN_MODEL'] = 'Qwen/Qwen2.5-Coder-7B-Instruct'\n",
    "print('QWEN_MODEL =', os.environ['QWEN_MODEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee511d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Qwen2.5-Coder and run a quick test\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "import torch, os\n",
    "\n",
    "model_id = os.environ.get('QWEN_MODEL', 'Qwen/Qwen2.5-Coder-1.5B-Instruct')\n",
    "\n",
    "# Device selection: prefer Apple Silicon MPS if available, else CPU\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device.type == 'mps' else torch.float32,\n",
    "    device_map=None\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "prompt = \"# Write a Python function to reverse a string with tests.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen2.5-Coder, a helpful coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# Qwen Instruct uses chat template; fall back to plain when missing\n",
    "if hasattr(tokenizer, 'apply_chat_template'):\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "else:\n",
    "    input_text = prompt\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors='pt').to(device)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "print('\\n\\nDone.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711bfd90",
   "metadata": {},
   "source": [
    "## Optional: Download with `huggingface-cli`\n",
    "\n",
    "If you prefer to pre-download the model (instead of letting Transformers fetch on first use), you can run in a terminal:\n",
    "\n",
    "```bash\n",
    "# Login (optional for gated/private; public models can be downloaded anonymously)\n",
    "huggingface-cli login\n",
    "\n",
    "# Download the 1.5B Instruct model to the cache\n",
    "huggingface-cli download Qwen/Qwen2.5-Coder-1.5B-Instruct --local-dir ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct --local-dir-use-symlinks False\n",
    "```\n",
    "\n",
    "To try larger variants, replace the repo name:\n",
    "- `Qwen/Qwen2.5-Coder-7B-Instruct`\n",
    "- `Qwen/Qwen2.5-Coder-32B-Instruct`\n",
    "\n",
    "For lower RAM via quantization, consider GGUF with llama.cpp (CPU/GPU offload)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5033e9f7",
   "metadata": {},
   "source": [
    "> Memory note: The 7B model can use ~14–18 GB RAM/VRAM depending on dtype. On Mac with MPS, it may still run but can be slower or OOM in low-memory situations. If you hit OOM, try:\n",
    "> - Close other apps; ensure enough swap free.\n",
    "> - Reduce `max_new_tokens` or use `temperature=0.0` for greedy.\n",
    "> - Switch to smaller: `Qwen/Qwen2.5-Coder-1.5B-Instruct`.\n",
    "> - Consider quantized GGUF via llama.cpp for lower RAM."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
